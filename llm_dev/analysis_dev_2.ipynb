{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import json\n",
    "\n",
    "import psycopg2\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "import tomllib\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from haystack import Document as DocumentH\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack_integrations.components.embedders.amazon_bedrock import (\n",
    "    AmazonBedrockDocumentEmbedder,\n",
    "    AmazonBedrockTextEmbedder,\n",
    ")\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(conversation, client, model_id):\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration\n",
    "        response = client.converse(\n",
    "                    modelId=model_id,\n",
    "                    messages=conversation,\n",
    "                    inferenceConfig={\"maxTokens\": 4096, \"temperature\": 0},\n",
    "                    additionalModelRequestFields={\"top_k\": 250, \"top_p\": 1},\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text\n",
    "        return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def append_prompt(template_dir):\n",
    "    with open(template_dir, \"rb\") as f:\n",
    "        settings = tomllib.load(f)\n",
    "\n",
    "    prompts = [settings['instruction_prompt'], \n",
    "               settings['task_prompt'], \n",
    "               settings['example_prompt'], \n",
    "               settings['reasoning_prompt']]\n",
    "    return \"\\n\".join(prompts)\n",
    "    \n",
    "\n",
    "def assemble_analysis_prompt(content, template_dir):\n",
    "    with open(template_dir, \"rb\") as f:\n",
    "        settings = tomllib.load(f)\n",
    "\n",
    "    message = [\n",
    "        {\"role\" : \"user\", \"content\" : [{\"text\" : settings['role_prompt']}, \n",
    "                                       {\"text\" : settings['task_prompt']},\n",
    "                                       {\"text\" : settings['example_prompt']},\n",
    "                                       {\"text\" : settings['reasoning_prompt']},\n",
    "                                       {\"text\" : settings['output_prompt']},\n",
    "                                       {\"text\" : f\">>>>>\\n{content}\\n<<<<<\"},\n",
    "                                       {\"text\" : settings['instruction_prompt']}]}\n",
    "    ]\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory\n",
    "directory_path = '../data/docs'\n",
    "\n",
    "# Find all PDF files recursively\n",
    "pdf_files = glob.glob(os.path.join(directory_path, '**', '*.pdf'), recursive=True)\n",
    "\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAZXNNZJEPQOQ6SCAT\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
    "\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "embedder_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_embedder_pipline(file_path, embedder_model_id):\n",
    "\n",
    "    document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    documents = [DocumentH(content = d.page_content, meta = d.metadata) for d in documents]\n",
    "\n",
    "    document_embedder = AmazonBedrockDocumentEmbedder(model=embedder_model_id, meta_fields_to_embed=[\"source\"])\n",
    "    documents_with_embeddings = document_embedder.run(documents)['documents']\n",
    "    document_store.write_documents(documents_with_embeddings)\n",
    "\n",
    "    query_pipeline = Pipeline()\n",
    "    query_pipeline.add_component(\"text_embedder\", AmazonBedrockTextEmbedder(model=embedder_model_id))\n",
    "    query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "    query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "\n",
    "    return query_pipeline, document_store\n",
    "\n",
    "def extract_relevent_and_prompt_llm(query_pipeline, template_dir, top_k = 10):\n",
    "    query = append_prompt(template_dir)\n",
    "\n",
    "    result = query_pipeline.run({\"text_embedder\":{\"text\": query}})\n",
    "\n",
    "    relevant_results = result['retriever']['documents'][:top_k]\n",
    "\n",
    "    relevant_results = sorted(relevant_results, key = lambda x : x.meta['page'])\n",
    "\n",
    "    content = \"\\n\\n -------------------- \\n\\n\".join([d.content for d in relevant_results])\n",
    "\n",
    "    analysis_prompt = assemble_analysis_prompt(content, template_dir)\n",
    "    response = query_llm(analysis_prompt, client, model_id)\n",
    "    try:\n",
    "        response_dict = json.loads(response) \n",
    "        response_dict['pages'] = [r.meta['page'] for r in relevant_results]\n",
    "\n",
    "        return response, response_dict\n",
    "    except:\n",
    "        print(\"Invalid format returned by LLM\")\n",
    "        return response\n",
    "    \n",
    "def llm_pipeline(query_pipeline, name):\n",
    "    analysis = dict(name = name)\n",
    "\n",
    "    response1 = extract_relevent_and_prompt_llm(query_pipeline, \"../templates/analysis_basic_indicators.toml\", top_k = 10)\n",
    "    analysis['basic'] = {'text' : response1} if len(response1) == 1 else {'text' : response1[0], 'obj' : response1[1]}\n",
    "    time.sleep(20)\n",
    "\n",
    "    response2 = extract_relevent_and_prompt_llm(query_pipeline, \"../templates/analysis_sector.toml\", top_k = 10)\n",
    "    analysis['sectore'] = {'text' : response2} if len(response2) == 1 else {'text' : response2[0], 'obj' : response2[1]}\n",
    "    time.sleep(20)\n",
    "\n",
    "    response3 = extract_relevent_and_prompt_llm(query_pipeline, \"../templates/analysis_sentiment.toml\", top_k = 10)\n",
    "    analysis['sentiment'] = {'text' : response3} if len(response3) == 1 else {'text' : response3[0], 'obj' : response3[1]}\n",
    "    time.sleep(20)\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Document_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30d8ba6ede94cf4afeb260cff4e7c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 208/208 [00:45<00:00,  4.54it/s]\n",
      "Creating embeddings: 100%|██████████| 208/208 [00:42<00:00,  4.86it/s]\n",
      "Creating embeddings: 100%|██████████| 216/216 [00:43<00:00,  4.93it/s]\n",
      "Creating embeddings: 100%|██████████| 196/196 [00:38<00:00,  5.06it/s]\n",
      "Creating embeddings: 100%|██████████| 196/196 [00:36<00:00,  5.31it/s]\n"
     ]
    }
   ],
   "source": [
    "document_store_save_path = \"../data/doc_store/\"\n",
    "\n",
    "report_names = []\n",
    "\n",
    "for file_path in tqdm(pdf_files):\n",
    "\n",
    "    file_path_splits = file_path.split(\"/\")\n",
    "    report_name = file_path_splits[-2] + \"_\" + file_path_splits[-1]\n",
    "    \n",
    "    query_pipeline, document_store = document_embedder_pipline(file_path, embedder_model_id)\n",
    "\n",
    "    document_store.save_to_disk(f\"{document_store_save_path}{report_name}.ds\")\n",
    "    report_names.append(report_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Document_stores and extract relevant chunks and pass to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231c156f54384ee6b061a0703fa25927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "ERROR: Can't invoke 'anthropic.claude-3-haiku-20240307-v1:0'. Reason: An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.\n",
      "Invalid format returned by LLM\n",
      "First try lead to Throttling error - sleeping for 120 seconds\n",
      "object of type 'NoneType' has no len()\n",
      "Second try started ...\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n",
      "Invalid format returned by LLM\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../data/doc_store/CPKC_CP_AnnualReport_2022.pdf.ds not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m file_path_splits \u001b[38;5;241m=\u001b[39m file_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m report_name \u001b[38;5;241m=\u001b[39m file_path_splits[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file_path_splits[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m document_store \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryDocumentStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdocument_store_save_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mreport_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.ds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m query_pipeline \u001b[38;5;241m=\u001b[39m Pipeline()\n\u001b[1;32m     16\u001b[0m query_pipeline\u001b[38;5;241m.\u001b[39madd_component(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embedder\u001b[39m\u001b[38;5;124m\"\u001b[39m, AmazonBedrockTextEmbedder(model\u001b[38;5;241m=\u001b[39membedder_model_id))\n",
      "File \u001b[0;32m~/Documents/HEC/3 - Polyfinance Datathon 2024/poly_datathon_2024/venv/lib/python3.11/site-packages/haystack/document_stores/in_memory/document_store.py:378\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.load_from_disk\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_object\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../data/doc_store/CPKC_CP_AnnualReport_2022.pdf.ds not found."
     ]
    }
   ],
   "source": [
    "cache_file_path = \"../data/cache.pkl\"\n",
    "\n",
    "document_store_save_path = \"../data/doc_store/\"\n",
    "\n",
    "with open(cache_file_path, \"rb\") as file:\n",
    "    cached_llm_results = pickle.load(file)\n",
    "\n",
    "for file_path in tqdm(pdf_files[23:]):\n",
    "\n",
    "    file_path_splits = file_path.split(\"/\")\n",
    "    report_name = file_path_splits[-2] + \"_\" + file_path_splits[-1]\n",
    "    \n",
    "    document_store = InMemoryDocumentStore.load_from_disk(f\"{document_store_save_path}{report_name}.ds\")\n",
    "\n",
    "    query_pipeline = Pipeline()\n",
    "    query_pipeline.add_component(\"text_embedder\", AmazonBedrockTextEmbedder(model=embedder_model_id))\n",
    "    query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "    query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "\n",
    "    try:\n",
    "        analysis = llm_pipeline(query_pipeline, report_name)\n",
    "    except Exception as e:\n",
    "        print(\"First try lead to Throttling error - sleeping for 120 seconds\")\n",
    "        print(e)\n",
    "        time.sleep(120)\n",
    "        try:\n",
    "            print(\"Second try started ...\")\n",
    "            analysis = llm_pipeline(query_pipeline, report_name)\n",
    "        except Exception as e:\n",
    "            print(\"Second try lead to Throttling error as well - sleeping for 600 seconds\")\n",
    "            time.sleep(600)\n",
    "            try:\n",
    "                print(\"Third try started ...\")\n",
    "                analysis = llm_pipeline(query_pipeline, report_name)\n",
    "            except:\n",
    "                print(\"Third try lead to Throttling error as well\")\n",
    "                print(f\"Ignoring report {report_name} for now and sleeping for 600 seconds\")\n",
    "                time.sleep(600)\n",
    "                continue\n",
    "\n",
    "    cached_llm_results.append(analysis)\n",
    "\n",
    "    with open(cache_file_path, \"wb\") as file:\n",
    "        pickle.dump(cached_llm_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
